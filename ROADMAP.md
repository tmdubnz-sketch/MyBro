# amo - Multiplatform Production Roadmap (2026 Standards)

## Multiplatform Production Requirements

To deploy **amo** as a production-grade multiplatform application in 2026, the following architectural and hardware requirements must be met.

### 1. Hardware & OS Requirements
*   **Mobile (iOS)**: iOS 18+ (Requires A15 Bionic or newer for stable WebGPU memory allocation). Safari 18+ with WebGPU enabled.
*   **Mobile (Android)**: Android 14+. Requires Vulkan-backed WebGPU support (Snapdragon 8 Gen 1 or newer recommended for 2B+ models).
*   **Desktop (Windows/macOS/Linux)**: Chrome 113+, Edge 113+, or Safari 18+. Minimum 4GB Unified Memory/VRAM for 0.5B-1.5B models; 8GB+ recommended for Gemma 2 2B.

### 2. Progressive Web App (PWA) Standards
*   **Service Worker Caching**: Model weights (ranging from 400MB to 1.6GB) must be cached using the Origin Private File System (OPFS) or IndexedDB via the MLC engine. The Service Worker (`workbox`) is configured to cache the WASM binaries and UI assets (up to 10MB chunks).
*   **Manifest**: Must include `display: standalone`, `theme_color`, and maskable icons for native OS integration.
*   **Installability**: App must pass Lighthouse PWA criteria, requiring HTTPS (Secure Context) which is mandatory for both WebGPU and `getUserMedia` (Microphone).

### 3. Audio & Voice Pipeline
*   **Input**: `getUserMedia` with echo cancellation, noise suppression, and auto-gain control enabled.
*   **Processing**: Migration from deprecated `ScriptProcessorNode` to `AudioWorklet` for main-thread-free audio processing (Targeted for Phase 2).
*   **Latency**: End-to-end latency for Gemini Live must remain under 300ms. Local chunked TTS must trigger within 50ms of the first punctuation mark generated by the WebGPU model.

### 4. Security & Privacy
*   **Local Models**: 100% of data remains on-device. No telemetry or network requests are made during local inference.
*   **Cloud Models**: Gemini API keys must be securely managed. In a shared production environment, users should authenticate via OAuth (e.g., Google/GitHub) and the backend should proxy requests to protect the master API key, or users must provide their own keys via a secure vault.

---

## Development Roadmap

### Phase 1: Foundation (Completed)
- [x] WebGPU Local Inference (Gemma 2, Qwen 2.5)
- [x] Gemini 2.5 Live Audio Integration
- [x] Responsive, Mobile-First UI
- [x] PWA Configuration for Desktop/Mobile Installation
- [x] Chunked TTS for low-latency local voice

### Phase 2: Enhanced Capabilities (Next Steps)
- [ ] **AudioWorklet Migration**: Replace `ScriptProcessorNode` in `geminiLiveService.ts` with `AudioWorklet` to prevent audio stuttering during heavy UI rendering.
- [ ] **Local RAG (Retrieval-Augmented Generation)**: Implement OPFS-backed SQLite vector database to allow users to upload PDFs and documents for the local model to read without sending data to the cloud.
- [ ] **Vision Capabilities**: Integrate WebGPU-based vision models (e.g., Moondream or Qwen-VL) to allow users to analyze images locally.
- [ ] **Thread Management**: Add a sidebar history to save, resume, and delete past conversations using IndexedDB.

### Phase 3: Native Packaging & Distribution
- [ ] **Desktop App**: Wrap the PWA using **Tauri v2** for lightweight, native desktop applications (Windows/macOS/Linux) with deep OS integration (e.g., global keyboard shortcuts to summon amo).
- [ ] **Mobile App Stores**: Use **Capacitor** or **Trusted Web Activities (TWA)** to package the application for the Apple App Store and Google Play Store.
- [ ] **Background Execution**: Implement background audio processing so amo can continue speaking or listening while the app is minimized on mobile devices.
